---
title: 'Computational Social Science: Sensitivity Analysis and Bounds'
author: "Your Name Here"
date: "MM/DD/YYYY"
output: pdf_document
---

```{r message=FALSE, echo=FALSE}
# Install packages 
if (!require("pacman")) install.packages("pacman")

pacman::p_load(# Tidyverse packages including dplyr and ggplot2 
               tidyverse,
               ggthemes,
               ATE,
               dagitty,
               ggdag,
               sensitivitymv,
               EValue)

theme_set(theme_dag())
source("pretty_dag.R")
set.seed(13)
```

# Sensitivity Analysis Rationale

So far in our explorations of observational studies we have considered various methods of accounting for measured confounders $W$ as well as some cases in which we can account for unmeasured confounders $U$ (e.g. if we have measured a suitable instrumental variable).

However, it is quite often the case that we are simply unable to control for unmeasured confounding in any meaningful way. Another strategy then is to quantify the *uncertainty* due to unmeasured confounding. Doing so is commonly referred to as Sensitivity Analysis. (Note: The term "Sensitivity Analysis" can be used much more broadly to refer to methods of varying *any* modeling assumptions, but here we use the more specific definition)

Recall that we already use methods of quantifying uncertainty due to random variation (e.g. standard error, confidence intervals, hypothesis testing). However, this *random* uncertainty is distinct from the *systematic* uncertainty due to unmeasured confounding.

# Simulation

Let us again consider a version of our AspiTyleCedrin example. Much like in our Instrumental Variables lab, in this version both exposure to AspiTyleCedrin and the outcome of experiencing a migraine are affected by watching cable news, since AspiTyleCedrin ads are commonly shown on cable news channels, and stress from excessive cable news watching can trigger migraines. 
However, in this case we have not been able to measure an instrumental variable such as living near a pharmacy which sells AspiTyleCedrin.

Thus we have the following variables:

**Endogenous variables:**

* `A`: Treatment variable indicating whether the individual $i$ took AspiTyleCedrin ($A_i = 1$) or not ($A_i = 0$).
* `Y`: Outcome variable indicating whether the individual experienced a migraine ($Y_{i_{obs}} = 1$) or not ($Y_{i_{obs}} = 0$).
* `W`: Variable representing sex assigned at birth, with $W = 0$ indicating AMAB (assigned male at birth), $W = 1$ indicating AFAB (assigned female at birth), and $W = 2$ indicating an X on the birth certificate, possibly representing an intersex individual or left blank.


**Exogenous variables:**

* `U`: Unmeasured confounding variable, cable news watching, which affects the exposure $A$ and the outcome $Y$, 

And our DAG is as follows:

```{r out.width="75%", echo=FALSE}
dagify(Y ~ A,
       Y ~ U,
       Y ~ W,
       A ~ U,
       A ~ W,
       exposure = "A",
       outcome = "Y") %>%
  tidy_dagitty() %>%
  pretty_dag() %>%
  ggdag() +
  geom_dag_edges() +
  geom_dag_node(aes(color = color)) +
  geom_dag_text(col = "white") +
  theme(legend.position = "none") +
  scale_color_manual(values=c("darkred", "lightgrey", "darkgrey", "navy"))
```

Simulate the dataset:

```{r}
n = 1e4 # Number of individuals

# NOTE: Again, don't worry too much about how we're creating this dataset, 
# this is just an example.

df <- data.frame(U = rbernoulli(n, p = 0.34),
                 W = sample(0:2, size = n, replace = TRUE, 
                             prob = c(0.49,0.50,0.01))
)

df <- df %>% 
  mutate(Y_0 = as.numeric(rbernoulli(n,
                                     p = (0.87 + 0.035*(W > 0) + 
                                            0.05*(U > 0)))),
         Y_1 = as.numeric(rbernoulli(n, 
                                     p = (0.34 + 0.035*(W > 0) + 
                                            0.3*(U > 0)))),
         A = as.numeric(rbernoulli(n, 
                                   p = (0.03 + 0.06*(W > 0) + 0.21*(U == 1)))),
         ITE = Y_1 - Y_0,
         Y = as.numeric((A & Y_1) | (!A & Y_0))
)

head(df)
summary(df)

ATE_true <- mean(df$ITE)
ATT_true <- mean((df %>% filter(A == 1))$ITE)

df <- df %>% select(A, W, Y)
```

Recall that the Average Treatment Effect (ATE) is the average difference in the pair of potential outcomes averaged over the entire population of interest (at a particular moment in time), or rather, it is just the average (or expected value) of the individual-level treatment effect.

$$\text{ATE} = E[Y_{i}(1) - Y_{i}(0)]$$

**\textcolor{blue}{Question 1:}** Use the `group_by()` and `summarize()` functions to find the estimated average treatment effect using the following formula. Compare this result to the actual ATE (saved as `ATE_true`).

$$\hat{\text{ATE}} = E[Y_i |A_i = 1, W_i = w_i] - E[Y_i|A_i = 0, W_i = w_i]$$
```{r}
est <- df %>% group_by(A) %>%
  summarise(E_Y = mean(Y))

ATE_crude <- est$E_Y[2] - est$E_Y[1]
```

$$\text{ATE} = ATE_{true}$$

$$\hat{\text{ATE}} \approx ATE_{crude}$$

The following code chunk uses the `R` package `ATE` to estimate the ATE in a similar manner, but also allows us to calculate confidence intervals and p-values for this estimate:

```{r}
ate_out <- ATE(df$Y, df$A, (df %>% select(W)))
summary(ate_out)
```

**\textcolor{blue}{Question 2:}** Report the estimate calculated by the `ATE()` function call above, as well as the 95% confidence interval and p-value. 

$$\hat{\text{ATE}} \approx summary(ate_{out})\$Estimate[3,1]$$
$$\text{95\% CI}_{\hat{\text{ATE}}} \approx [summary(ate_{out})\$Estimate[3,3], summary(ate_{out})\$Estimate[3,4]] $$
$$\text{p-value} \approx summary(ate_{out})\$Estimate[3,6]$$


**\textcolor{blue}{Question 3:}** What would you conclude from the information you reported in the previous question if you did not know the true ATE? From what you do know the true ATE and the DAG, do you think the confidence interval appropriately captures the level of uncertainty of that estimate? Explain.

> These results show a very narrow confidence interval and a tiny p-value, which would generally lead us to be very confident in this estimate. However, we can see the true ATE value is not included in this CI, and furthermore we know from the DAG that there is unmeasured confounding $U$ that we have not controlled for.

This is a stark reminder that our interpretation of measures such as p-values, CIs, etc. are only valid if our assumptions are correct! Here we know they are not.

The CI and p-value calculated above account only for the random uncertainty (and systematic uncertainty from measured confounding $W$. Now we will consider different methods of quantifying the uncertainty due to the *unmeasured confounding $U$*.

# Manski Bounds

One method for quantifying the uncertainty due to the unmeasured confounding $U$ is by quantifying logical bounds upon necessary parameters and propagating those bounds through to the parameter of interest.

First, let us consider the ATE in more detail. We can rewrite the formula from above as:

$$\text{ATE} = E[Y_{i}(1) - Y_{i}(0)] = E[Y_{i}(1)] - E[Y_{i}(0)] = \mu_t - \mu_c$$

where $\mu_t$ is simply the average outcome under the counterfactual in which everyone received the treatment ($A = 1$ for everyone) and $\mu_c$ is simply the average outcome under the counterfactual in which no one received the treatment ($A = 0$ for everyone). NOTE: For simplicity we will ignore $W$ for now.

Note that by the definition of expectation we can further break down these $\mu$ values like so:

\begin{align*}
\mu_t &= P(A = 1) \cdot E[Y_{i}(1) | A = 1]  + P(A = 0) \cdot E[Y_{i}(1) | A = 0] \\
\mu_c &= P(A = 1) \cdot E[Y_{i}(0) | A = 1]  + P(A = 0) \cdot E[Y_{i}(0) | A = 0] \\
\end{align*}

Or:

\begin{align*}
\mu_t &= p \cdot \mu_{t,1} + (1-p)\cdot \mu_{t,0} \\
\mu_c  &= p \cdot \mu_{c,1} + (1-p)\cdot \mu_{c,0} \\
\end{align*}

where:

* $p$ is the probability of treatment.
* $\mu_{t,1}$ is the average outcome of treatment among those who actually receive the treatment.
* $\mu_{t,0}$ is the average outcome of treatment among those who *do not* receive the treatment.
* $\mu_{c,1}$ is the average outcome of *not receiving* treatment among those who actually receive the treatment.
* $\mu_{c,0}$ is the average outcome of *not receiving* treatment among those who *do not* receive the treatment.

In practice, we can reasonably estimate $p$, $\mu_{t,1}$ and $\mu_{c,0}$, but *not* $\mu_{t,0}$ and $\mu_{c,1}$.

**\textcolor{blue}{Question 4:}** Explain why the previous statement is true.

> We can estimate $p$ from the proportion of treated individuals in our dataset. Furthermore, our observed data does contain the outcome of treatment among individuals receiving the treatment and the outcome of no treatment among individuals receiving the treatment, but it is impossible for us to observe what would happen to individuals under the opposite treatment condition than actually happened.

However, we do know that since the outcome $Y$ is binary, $\mu_{t,0}$ and $\mu_{c,1}$ must by definition lie inside the interval [0,1]. This knowledge allows us to construct bounds on $\mu_t$ and $\mu_c$ like so:

\begin{align*}
\mu_t &\in [p \cdot \mu_{t,1} , p \cdot \mu_{t,1} + (1-p) ]\\
\mu_c  &\in [ (1-p) \cdot \mu_{c,0} , (1-p) \cdot \mu_{c,0} + p] \\
\end{align*}

**\textcolor{blue}{Question 5:}** Show how the bounds for $\mu_t$ and $\mu_c$ above follow from bounds of [0,1] on $\mu_{t,0}$ and $\mu_{c,1}$.

\begin{align*}
LL_{\mu_t} &= \mu_t, [\mu_{t,0} = 0] \\
&= p \cdot \mu_{t,1} + (1-p)\cdot 0 \\
&= p \cdot \mu_{t,1} \\
UL_{\mu_t} &= \mu_t, [\mu_{t,0} = 1] \\
&= p \cdot \mu_{t,1} + (1-p)\cdot 1 \\
& = p \cdot \mu_{t,1} + (1-p) \\
LL_{\mu_c}  &= \mu_c, [\mu_{c,1} = 0] \\
&= p \cdot 0 + (1-p)\cdot \mu_{c,0}  \\
&= (1-p)\cdot \mu_{c,0} \\
UL_{\mu_c}  &= \mu_c, [\mu_{c,1} = 1] \\
&= p \cdot 1 + (1-p)\cdot \mu_{c,0}  \\
&= p + (1-p)\cdot \mu_{c,0} \\
\end{align*}

We can then use these bounds of $\mu_t$ and $\mu_c$ in the formula for the ATE to get bounds for the ATE itself.

\begin{align*}
\text{ATE} &= \mu_t - \mu_c \\
LL_{ATE} &= LL_{\mu_t} - UL_{\mu_c} \\
&= [p \cdot \mu_{t,1}] - [p + (1-p)\cdot \mu_{c,0}]  \\
UL_{ATE} &= UL_{\mu_t} - LL_{\mu_c} \\
&= [p \cdot \mu_{t,1} + (1-p)] - [(1-p)\cdot \mu_{c,0}]  \\
\end{align*}

**\textcolor{blue}{Question 6:}** Why are the bounds for the ATE calculated in this way? For example, why not find the difference of both lower bounds and both upper bounds, respectively?

> The ATE is the *difference* in the average outcome among the treatment and control, so this difference will be smallest when the average outcome of the treatment is high and average outcome of the control is low (because they will be closer to each other), and vice versa for the upper bound.

We now have a formula for the ATE using only $p$, $\mu_{t,1}$ and $\mu_{c,0}$, which we said earlier can be reasonably estimated from our data. Thus we can use plug-in estimators for these values to estimate the bounds for the ATE, like so:

\begin{align*}
\hat{LL}_{ATE} &= [\hat{p} \cdot \hat{\mu}_{t,1}] - [\hat{p} + (1 - \hat{p}) \cdot \hat{\mu}_{c,0}]  \\
\hat{UL}_{ATE} &= [\hat{p} \cdot \hat{\mu}_{t,1} + (1 - \hat{p})] - [(1 - \hat{p})\cdot \hat{\mu}_{c,0}]  \\
\end{align*}

**\textcolor{blue}{Question 7:}** Calculate these bounds on the ATE using the data.

```{r}
p_hat <- nrow(df %>% filter(A == 1)) / nrow(df)
mu.t1_hat <- df %>% filter(A == 1) %>% summarise(mean = mean(Y)) %>% pull(mean)
mu.c0_hat <- df %>% filter(A == 0) %>% summarise(mean = mean(Y)) %>% pull(mean)

LL_ATE <- p_hat*mu.t1_hat - (p_hat + (1 - p_hat)*mu.c0_hat)
UL_ATE <- p_hat*mu.t1_hat + (1 - p_hat) - (1 - p_hat)*mu.c0_hat
```

$$ATE \in [`LL_{ATE}`,`UL_{ATE}`]$$
Note that by definition this interval must contain zero and must have a width of one. Do not confuse this with a confidence interval! Also note that this does not include the random variation associated with our estimates.

**\textcolor{blue}{Question 8:}** Compare this interval to the true ATE.

> The interval does indeed include the true ATE value. What's more, while the interval crosses zero, it clearly has more negative range than positive, which is encouraging considering the true value is in fact negative.

# Rosenbaum Sensitivity Analysis

While Manski Bounds are useful to estimate a "worst case" range of possible values, since zero is necessarily included they are not especially informative.

It is useful, then, to be able to use outside knowledge or other reasonable assumptions about possible ranges of values to possibly restrict this interval.

The Rosenbaum-Rubin approach incorporates a bit more complexity than do Manski bounds. First, we imagine the unmeasured confounding $U$ as binary with some probability $q$, where:

$$q = P(U_i = 1) = 1 - P(U_i = 0)$$
We then specify models for the relationships between the unmeasured confounding $U$ and the other variables: treatment assignment $A$, outcome under treatment $Y(1)$, and outcome under no treatment $Y(1)$. Note that we are still ignoring the measured confounder $W$ for now. Since $A$, $Y(1)$, and $Y(0)$ are all binary, we can specify logistic models for these relationships, like so:

\begin{align*}
\text{logit}[P(A_i = 1 | U_i = u)] &= \gamma_0 + \gamma_1 \cdot u \\
\text{logit}[P(Y_i(1) = 1 | U_i = u)] &= \alpha_0 + \alpha_1 \cdot u \\
\text{logit}[P(Y_i(0) = 1 | U_i = u)] &= \beta_0 + \beta_1 \cdot u \\
\end{align*}

Or:

\begin{align*}
P(A_i = 1 | U_i = u) &= \text{logit}^{-1}[\gamma_0 + \gamma_1 \cdot u] \\  
&= \frac{e^{\gamma_0 + \gamma_1 \cdot u}}{1 + e^{\gamma_0 + \gamma_1 \cdot u}} \\
P(Y_i(1) = 1 | U_i = u) &= \text{logit}^{-1}[\alpha_0 + \alpha_1 \cdot u]  \\  
&= \frac{e^{\alpha_0 + \alpha_1 \cdot u}}{1 + e^{\alpha_0 + \alpha_1 \cdot u}} \\
P(Y_i(0) = 1 | U_i = u) &= \text{logit}^{-1}[\beta_0 + \beta_1 \cdot u]  \\  
&= \frac{e^{\beta_0 + \beta_1 \cdot u}}{1 + e^{\beta_0 + \beta_1 \cdot u}} \\
\end{align*}

Now we recall that we can write the ATE as:

\begin{align*}
ATE &= \mu_t - \mu_c \\
&= [p \cdot \mu_{t,1} + (1-p)\cdot \mu_{t,0}] - [p \cdot \mu_{c,1} + (1-p)\cdot \mu_{c,0}] \\
&= p \cdot (\mu_{t,1} - \mu_{c,1}) + (1-p)\cdot ( \mu_{t,0} - \mu_{c,0}) \\
\end{align*}

Following the derivations in Chapter 22 of Imbens/Rubin, we can write each of these parameters ($p$, $\mu_{t,1}$, $\mu_{t,0}$, $\mu_{c,1}$, $\mu_{c,0}$) in terms of the parameters seen above ($q$, $\gamma_0$, $\gamma_1$, $\alpha_0$, $\alpha_1$, $\beta_0$, $\beta_1$). 

Chapter 22.4 details the translations of our reasonably estimable values $p$, $\mu_{t,1}$, and $\mu_{c,0}$, and shows that these relationships can allow us to find estimate values for $\gamma_0$, $\alpha_0$, and $\beta_0$.

If we then postulate values for $q$, $\gamma_1$, $\alpha_1$, and $\beta_1$, we can then estimate values for $\mu_{t,0}$ and $\mu_{c,1}$. Our sensitivity analysis then comes down to the decisions we make for postulating $q$, $\gamma_1$, $\alpha_1$, and $\beta_1$.

For example, if we fix $q = p$ and let $\gamma_1 \rightarrow \infty$, $\alpha_1 \rightarrow \infty$, and $\beta_1 \rightarrow \infty$, we find:

$$ATE = p \cdot \mu_{t,1} - p - (1 - p) \cdot \mu_{c,0}$$
which is equivalent to the lower limit derived from the Manski bounds! Furthermore, if we fix $q = p$ and let $\gamma_1 \rightarrow \infty$, $\alpha_1 \rightarrow -\infty$, and $\beta_1 -\rightarrow \infty$, we find:

$$ATE = p \cdot \mu_{t,1} + (1 - p) - (1 - p) \cdot \mu_{c,0}$$
which is equivalent to the upper limit derived from the Manski bounds! Therefore we can see Manski bounds as simply a special case of this type of sensitivity analysis.

## Shiny App

This approach has been implemented via a Shiny App [here](https://carohaensch.shinyapps.io/tippingsens/). This app allows you to upload your data and adjust two of the parameters discussed above ($q$, $\gamma_1$, $\alpha_1$, $\beta_1$) while holding the other two constant.

**\textcolor{blue}{Question 9:}** Upload our data to the app and play around with a few different values/ranges of the four parameters. Take a screesnot of at least one of your iterations and include it below. Discuss what you see and interpret in terms of our original analysis. (Note: You will need to modify the format of our dataset slightly before uploading--read instructions on the app webpage for details)

```{r}
df.shiny <- df %>% 
  mutate(Treatment = A,
         Outcome = Y) %>%
  select(-W)

write.csv(df.shiny, "df.shiny.csv")
```

# E-Value

The last technique we will look at is the E-Value. The basic logic of the E-value is that it is the necessary strength of association between an unobserved confounder would need with both the treatment and outcome to erase an effect estimate. A small E-value implies only a small amount of confounding is necessary to erase the results, whereas a large E-value implies a large amount of confounding would be necessary.

To run the E-value calculation, we can use the [`EValue`](https://cran.r-project.org/web/packages/EValue/index.html) package. THe main function we will look at is `evalues.RR()` which evaluates the E-Value using a risk ratio (the probability of an outcome occurring in the exposed group relative to the probability of an outcome in the unexposed group). To run this analysis we simply need this one line:

```{r}
evalues.RR(est = 0.3434709, lo = 0.3152002, hi =0.3717417)
```
Using our ATE estimates from earlier, we see that we get an E-value of 5.27. This E-value implies that a confounder would need to be associated with a 5.27-fold increase in the individual experiencing a migraine, and be 5.27 times more prevalent among people who received the drug. 

**Question**: Do you find this figure to be reassuring?

**Answer**: Probably not, it is easy to imagine an exogenous variable that increases the likelihood someone both takes the drug and develops a migraine anyway, and a 5-fold increase in this risk is not especially high. Like with p-values though, we should be careful about using arbitrary cutoffs to determine whether our results are valid. 


# References

* Imbens, G. W., &amp; Rubin, D. B. (2018). Causal inference for statistics, social, and biomedical sciences: An introduction. New York: Cambridge University Press.

* https://carohaensch.shinyapps.io/tippingsens/