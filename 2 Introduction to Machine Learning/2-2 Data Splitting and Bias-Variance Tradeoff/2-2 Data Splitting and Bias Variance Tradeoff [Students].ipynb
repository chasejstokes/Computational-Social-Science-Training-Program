{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Computational Social Science] \n",
    "\n",
    "## 2-2 Data Splitting and Bias/Variance Tradeoff - Student Version\n",
    "\n",
    "In this lab, we will introduce the fundamentals of machine learning. We will introduced supervised methods, bias/variance tradeoff, and data splitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by introducing the concept of supervised learning. We're going to work with [Census Income dataset](https://archive.ics.uci.edu/ml/datasets/Census+Income) on the UCI Machine Learning Repository. In supervised machine learning, we start with a structured dataset where each observation is a row and each variable is a column. The goal is to use a number of features (previously you might have called these \"covariates,\" \"independent variables,\" or \"regressors\") to train a model that predicts a target (previously you might have called these \"outcome variable\" or \"dependent variable\"). In this case, we will use information like age, education, and marital status to predict each person's income-bracket.\n",
    "\n",
    "First let's load the data. The data is stored in a \".data\" format so we'll use the pandas \"read_table\" method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of column names, found in \"adult.names\"\n",
    "col_names = ['age', 'workclass', 'fnlwgt',\n",
    "            'education', 'education-num',\n",
    "            'marital-status', 'occupation', \n",
    "             'relationship', 'race', \n",
    "             'sex', 'capital-gain',\n",
    "            'capital-loss', 'hours-per-week',\n",
    "            'native-country', 'income-bracket']\n",
    "\n",
    "# Read table from the data folder\n",
    "census = pd.read_table(\"../../data/adult.data\", sep = ',', names = col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the first 5 rows\n",
    "census.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we predict income-bracket, we need to think about a few questions.\n",
    "\n",
    "1. What features should we include?\n",
    "2. What is the best model?\n",
    "3. How do we evaluate the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's think about the features. Check the different data types in our dataset. What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we pass these features into a model, we need to convert the categorical features to numerical information. Our target is also categorical, so we will need to transform that column as well. Starting with the outcome, income-bracket, use sklearn's [LabelBinarizer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelBinarizer.html) to convert the **income-bracket** column to a binary outcome (0 for \"<=50k\" and 1 for \">50k\"). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer \n",
    " \n",
    "lb = LabelBinarizer()\n",
    "census['income-bracket-binary'] = lb.fit_transform(census[\"income-bracket\"]) # use fit_transform to create new variable\n",
    "census.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the features we will use for prediction. There are a couple of different ways we could convert our categorical features to numerical ones. One simple way to do this is to convert them into dummy variables. Use the pandas [get_dummies](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html) method to convert the categorical variables into dummy variables. *Be sure to remove the target variable (both the original and binarized versions) with* `.drop()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create X by dropping our y variable from the df with the .drop() method. which axis refers to columns?\n",
    "X = \n",
    "\n",
    "# use get_dummies method to convert selected vars into dummy variables\n",
    "X = pd.get_dummies(...)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that all of our categorical variables have been converted. Instead of strings, we now either have a 0 or a 1 in their place, and new columns representing each category. Take a look at the shape and structure of this new dataframe compared to the original. What can you say about these two dataframes? Is there new information, or is it presented differently? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dimensions of Dummy Dataframe are\", X.shape)\n",
    "print(\"Dimensions of the Original Dataframe are\", census.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curse of Dimensionality and Sparsity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data preprocessing reveals two important concepts in machine learning. Namely, [**the curse of dimensionality** and **sparsity**](https://www.kaggle.com/residentmario/curse-of-dimensionality). A sparse matrix is one filled mainly with 0s, as we see above. In geometric terms, this property means that most combinations of features are totally empty. The curse of dimensionality refers to the idea that as the number of features grow, the number of observations needed to properly model predictions grows as well. The problem is that that number of observations required grows much faster than the number of features. Trading off between increasing features to improve model performance, but reducing the number of features to avoid the instability and overfitting that occurs with too few observations, is at the heart of supervised machine learning. In the coming weeks, we will learn more about how to conceptualize and make these tradeoffs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another key aspect of machine learning is splitting our data. In your previous statistics classes, you likely fit regression models on an entire dataset. The problem with fitting a model this way is that models will tend to perform better on the data that they are trained on, but then perform less well on out of sample data. The error in a machine learning model comes from two sources:\n",
    "\n",
    "**Bias**: Error arising from the difference between the predicted output and the actual output. High bias models tend to be too simple and **underfit** the dataset.\n",
    "\n",
    "**Variance**: Error arising from modeling the noise in the output. High variance models tend to be too complex and **overfit** the dataset.\n",
    "\n",
    "The [**bias-variance tradeoff**](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff) is a fundamental concept in machine learning. The less bias in a model, the higher the variance and vice versa. [Underfitting or overfitting](https://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html) a model makes generalizing to new data difficult. Trading off between these two sources of error is an essential part of machine learning. Over the next few weeks, we will explore how to deal with bias-variance tradeoff in the modeling process. Today, we'll look at how our choices in the data splitting step affect bias-variance tradeoff as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test/Validation Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, machine learning models will tend to overfit the data they are trained on. As a machine learning model becomes more complex, it learns many of the idiosyncrasies in a dataset, but this tendency will mean it generalizes poorly. To assess the extent a model is prone to this problem, and make corrections, we always split our data before training our models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general procedure for this is the following:\n",
    "1. Randomly divide our data set into two smaller sets: one for training and one for testing\n",
    "2. Train the data on the training set, changing our model along the way to increase accuracy\n",
    "3. Test the data's predictions using in test set.\n",
    "\n",
    "Scikit-learn's [test_train_split function](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) will help here. First, separate your data into two parts: a dataframe containing the features used to make our prediction, and an array of the true targets. We already made a dataframe with our features earlier when we created dummy variables, so now we need to make a target vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, set a random seed to ensure that we all get the same results when we sample. Then, use train_test_split to create a training set that contains 80% of the original dataset and a test set with 20% of the original data. Then check the shapes of each of the new sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# set the random seed\n",
    "np.random.seed(10)\n",
    "\n",
    "# split the data\n",
    "# train_test_split returns 4 values: X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of X train is\", ...)\n",
    "print(\"Shape of y_train is\", ...)\n",
    "print(\"Shape of X_test is\", ...)\n",
    "print(\"Shape of y_test is\", ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we may want to adjust our models, it is generally a good idea to save the test set until the very end and only use it once. Instead of going back and forth between the training and test set, we should instead use a validation set. Try using the `train_test_split` method to further split your training data so that 75% remains in training and 25% is reserved for validation. Note that this will mean the final split is 60% train, 20% validation, and 20% test. Check the dimensions of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_validate, y_train, y_validate = train_test_split(X_train, y_train,\n",
    "                                                    train_size=0.75, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use .shape to check out how many rows and columns you have in your train and test data\n",
    "print(\"Shape of X train is\", ...)\n",
    "print(\"Shape of y_train is\", ...)\n",
    "print(\"Shape of X_validate is\", ...)\n",
    "print(\"Shape of y_validate is\", ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to fit a model. We'll use a logistic regression and how accurate our model is in the validation set. We'll explore fitting multiple models and adjusting them in coming weeks, but for now we'll start with one that you're familiar with. See the documentation for [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) to see how to train a model and predict on new data. Make sure you train the model on your train set, and predict on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# create a model\n",
    "log_reg = ...\n",
    "\n",
    "# fit the model - learning the relationship between features (x_train) and labels (y_train)\n",
    "log_model = ...\n",
    "\n",
    "y_pred = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualize how we did. A common tool to see how a supervised machine algorithm performed in a classification setting is the [confusion matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html). Each cell in the confusion matrix shows the relationship between the true observations on the y-axis and the predicted observations on the x-axis. Cells along the diagonal are instances where the predictions match the true labels.\n",
    "\n",
    "Call Scikit-Learn's `confusion_matrix` using the y_validate and y_pred variables. Have a look in the documentation at the `normalize` parameter as well, and how it influences the visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cf_matrix = ...\n",
    "\n",
    "# Seaborn Plot for Confusion Matrix\n",
    "\n",
    "df_cm = pd.DataFrame(cf_matrix, range(2),\n",
    "                  range(2))\n",
    "\n",
    "df_cm = df_cm.rename(index=str, columns={0: \"<=50k\", 1: \">50k\"})\n",
    "df_cm.index = [\"<=50k\", \">50k\"]\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.set(font_scale=1.4)#for label size\n",
    "sns.heatmap(df_cm, \n",
    "           annot=True,\n",
    "           annot_kws={\"size\": 16},\n",
    "           fmt='g')\n",
    "\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Using the [iris](https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html) dataset, write a for loop that varies the training set size and visualize how performance changes as you increase the training size. What tends to happens? As a bonus, you can also use the census dataset that we've been working with so far, but it may take a while to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first load in and assess the df \n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "X_iris = iris.data[:, 0:2]  # Let's use the first two features\n",
    "y_iris = iris.target\n",
    "\n",
    "iris.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = np.arange(.2, 1, .10)\n",
    "\n",
    "for size in sizes:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(...)\n",
    "\n",
    "    # Initialize model\n",
    "    log_reg = ...\n",
    "    \n",
    "    # fit the model\n",
    "    log_model = ...\n",
    "\n",
    "    y_pred = ...\n",
    "\n",
    "    cf_matrix = ...\n",
    "    \n",
    "    df_cm = ...\n",
    "\n",
    "    # rename and index...can find the key to this data set using the target and target_names attributes\n",
    "    df_cm = df_cm.rename(index=str, \n",
    "                         columns={0: \"Setosa\", 1: \"Versicolor\", 2: \"Virginica\"})\n",
    "    df_cm.index = [\"Setosa\", \"Versicolor\", \"Virginica\"]\n",
    "    \n",
    "    plt.figure(figsize = (10,7))\n",
    "    sns.set(font_scale=1.4)#for label size\n",
    "    sns.heatmap(df_cm, \n",
    "               annot=True,\n",
    "               annot_kws={\"size\": 16},\n",
    "               fmt='g')\n",
    "\n",
    "    plt.title(\"Training Set Size \" + str(size))\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, choosing the optimal train/validation/test split can be difficult. It is also prone to high variance problems as the machine learning algorithm's performance will be very dependent on the composition of the randomly sampled test split. This problem is exacerbated in small datasets especially. One way to address this problem is with [cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)#:~:text=Cross%2Dvalidation%2C%20sometimes%20called%20rotation,to%20an%20independent%20data%20set).\n",
    "\n",
    "The general procedure is:\n",
    "\n",
    "1. Randomly split the data into k-folds\n",
    "2. Build the model on k-1 folds, then test on the last fold\n",
    "3. Record prediction error\n",
    "4. Cycle until each fold has served as the test set\n",
    "5. The average of the errors is the cv-error\n",
    "\n",
    "Cross-validation has the advantage of allowing every data point to be in the test set once. By averaging the errors, the model is less sensitive to variation in the random samples, and is less prone to overfitting. Let's try with our logistic regression again, using the [cross_val_predict](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_predict.html) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "X = census.drop(['income-bracket', 'income-bracket-binary'], axis = 1)\n",
    "X = pd.get_dummies(X)\n",
    "\n",
    "y = census['income-bracket-binary']\n",
    "\n",
    "log_pred = ...\n",
    "\n",
    "cf_matrix = confusion_matrix(y, log_pred, normalize = \"true\")\n",
    "\n",
    "df_cm = pd.DataFrame(cf_matrix, range(2),\n",
    "              range(2))\n",
    "\n",
    "df_cm = df_cm.rename(index=str, columns={0: \"<=50k\", 1: \">50k\"})\n",
    "df_cm.index = [\"<=50k\", \">50k\"]\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.set(font_scale=1.4)#for label size\n",
    "sns.heatmap(df_cm, \n",
    "           annot=True,\n",
    "           annot_kws={\"size\": 16},\n",
    "           fmt='g')\n",
    "\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Authored by Aniket Kesari. Updated by Tom van Nuenen in 2022."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
